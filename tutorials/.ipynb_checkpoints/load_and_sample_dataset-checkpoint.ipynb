{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bca11a49-551d-49bb-970b-460313d13c22",
   "metadata": {},
   "source": [
    "# Tutorial: Load and Sample Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4c13d-af8e-4dd4-bdc0-8701922cc757",
   "metadata": {},
   "source": [
    "In this tutorial you will learn what are the different ways to load datasets, subsample them.\n",
    "\n",
    "To run all cells, you need the `annotated-trainval` and `synthetic` datasets unzipped in `data/docile` and `annotated-trainval.zip` file in `data/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "daf865fb-be3d-4243-b261-7b6b8c46bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from docile.dataset import CachingConfig, Dataset\n",
    "\n",
    "DATASET_PATH = Path(\"/data/docile/\")\n",
    "DATASET_PATH_ZIP = Path(\"/data/annotated-trainval.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e17c01c-9d6f-48d6-9bff-02f73244bdfd",
   "metadata": {},
   "source": [
    "## Load from folder with unzipped dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93784f80-98e2-4375-9df0-3e0dcec0fd51",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index file for split val does not exist and no docids were passed.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m val \u001b[38;5;241m=\u001b[39m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATASET_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/SiaTests/docile/conda-env/lib/python3.8/site-packages/docile/dataset/dataset.py:68\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[0;34m(self, split_name, dataset_path, load_annotations, load_ocr, cache_images, docids)\u001b[0m\n\u001b[1;32m     66\u001b[0m docids_from_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_docids_from_index(split_name)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m docids_from_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex file for split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist and no docids were passed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m     )\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m docids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m docids_from_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m docids \u001b[38;5;241m!=\u001b[39m docids_from_file:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     73\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassed docids do not match the content of the index file for split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Index file for split val does not exist and no docids were passed."
     ]
    }
   ],
   "source": [
    "val = Dataset(\"val\", DATASET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b26393b-dd4a-42e2-87fb-7fc154d1efab",
   "metadata": {},
   "source": [
    "## Load from zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f82f329-f545-4303-a4de-720ef8ecd66e",
   "metadata": {},
   "source": [
    "Dataset can be loaded directly from zip as well but image caching to disk must be turned off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09ffdb3-93bc-4ffc-9b13-d4770157fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = Dataset(\"val\", DATASET_PATH_ZIP, cache_images=CachingConfig.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77805d5-513e-45b3-aa29-d721a8315a76",
   "metadata": {},
   "source": [
    "## Preloading document resources to memory and image caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6070d-59f4-41e4-976c-a69262638e24",
   "metadata": {},
   "source": [
    "By default, dataset is loaded with these settings:\n",
    "\n",
    "* annotations and pre-computed OCR are loaded to memory\n",
    "* images generated from PDFs are cached to disk (for faster access of the images in future iterations)\n",
    "\n",
    "Below you see options how to change this default behaviour, which is especially useful for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ad98f3-ca23-4abb-8d40-075c396bed2a",
   "metadata": {},
   "source": [
    "**Only load annotations, not pre-computed OCR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdbdca9-5c01-4975-ab6f-850e333a2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Dataset(\"train\", DATASET_PATH, load_ocr=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce69236-b160-479b-8d34-892f30550d72",
   "metadata": {},
   "source": [
    "**Postpone loading of document resources**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1bdfe1-c42d-4d06-8566-b15bdec8ab65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not preload document resources\n",
    "synthetic = Dataset(\"synthetic\", DATASET_PATH, load_annotations=False, load_ocr=False, cache_images=CachingConfig.OFF)\n",
    "# You can load part of the dataset later\n",
    "synthetic_sample = synthetic[:100].load()\n",
    "# And release it from memory later\n",
    "synthetic_sample.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0725f950-4cf3-427a-a6b6-1bedda063f95",
   "metadata": {},
   "source": [
    "**Cache images to both disk and memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ee58b4-8b6d-4a9e-9f9b-858a0403be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache images also in memory. Make sure you have enough RAM memory to do this!\n",
    "# Images are not loaded to memory right away but only after first\n",
    "train = Dataset(\"train\", DATASET_PATH, cache_images=CachingConfig.DISK_AND_MEMORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8456a0-7a3c-42f0-b368-dfb1bbbc6b29",
   "metadata": {},
   "source": [
    "## Sample and chunk documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb21a02-da04-4aa8-9ae0-d430f9fc9a82",
   "metadata": {},
   "source": [
    "For experiments and to work with large datasets, it can be useful to take samples of the datasets.\n",
    "\n",
    "For this, you can use slicing `[start:end:step]`, `.sample()`, `.get_cluster()` or `.from_documents()` methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69741c7-4b10-4409-86e9-2986a59b3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic = Dataset(\"synthetic\", DATASET_PATH, load_annotations=False, load_ocr=False, cache_images=CachingConfig.OFF)\n",
    "trainval = Dataset(\"trainval\", DATASET_PATH, load_annotations=False, load_ocr=False, cache_images=CachingConfig.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9621d77e-5820-4b2b-967f-98771468c07d",
   "metadata": {},
   "source": [
    "**Slicing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9cab7d-ac31-4cd3-a3c0-07ae91001cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic document has 100 chunks of 1000 documents from the same template document, so the\n",
    "# following line selects 1 document for each template document:\n",
    "synthetic_slice = synthetic[8::1000]\n",
    "print(synthetic_slice.docids[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788090cc-d536-4cc3-8df3-9e43acd4d598",
   "metadata": {},
   "source": [
    "**Random sample**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20041b46-49f5-475c-9fcb-ff2dea15ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_sample = synthetic.sample(5)\n",
    "print(synthetic_sample)\n",
    "print(synthetic_sample.docids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263df176-3366-4a25-8ee3-34d722ef0c79",
   "metadata": {},
   "source": [
    "**Documents belonging to the same cluster**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3026aabd-c303-4e0a-aced-4d6501b8a4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_cluster = trainval.get_cluster(synthetic[0].annotation.cluster_id)\n",
    "print(f\"Found {len(trainval_cluster)} documents in {trainval_cluster}.\")\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "print(\"Showing 10 images from the cluster:\")\n",
    "imgs = [doc.page_image(page=0, image_size=(None, 100)) for doc in trainval_cluster[:10]]\n",
    "concat_img = Image.new(\"RGB\", (sum(img.width for img in imgs), 100))\n",
    "start_from = 0\n",
    "for img in imgs:\n",
    "    concat_img.paste(img, (start_from, 0))\n",
    "    start_from += img.width\n",
    "concat_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6928e572-09c7-4637-b2a2-40f2e2967950",
   "metadata": {},
   "source": [
    "**Using custom filter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724681f-9a13-4a8a-bad9-5c10b3473a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval_ucsf = Dataset.from_documents(\"trainval-ucsf\", [doc for doc in trainval if doc.annotation.source == \"ucsf\"])\n",
    "trainval_pif = Dataset.from_documents(\"trainval-pif\", [doc for doc in trainval if doc.annotation.source == \"pif\"])\n",
    "print(f\"{trainval_ucsf} with {len(trainval_ucsf)} documents\")\n",
    "print(f\"{trainval_pif} with {len(trainval_pif)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999723a-e164-4bf4-9409-bcc68a458989",
   "metadata": {},
   "source": [
    "## Chunk dataset into parts with the same number of pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf55f32c-1fc9-4291-be19-c1c3d6a384e6",
   "metadata": {},
   "source": [
    "Create dataset chunks that have a limited number of pages. This can be especially useful for large datasets, such as the unlabeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bed2781-b19e-4ce5-9270-0c000a2647a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable\n",
    "\n",
    "def chunk_dataset(dataset: Dataset, max_pages_per_chunk: int) -> Iterable[Dataset]:\n",
    "    start_doc_i = 0\n",
    "    pages = 0\n",
    "    for doc_i, document in enumerate(dataset.documents):\n",
    "        documents = doc_i - start_doc_i + 1\n",
    "        pages += document.page_count\n",
    "        if doc_i > start_doc_i and pages > max_pages_per_chunk:\n",
    "            yield dataset[start_doc_i:doc_i]\n",
    "            start_doc_i = doc_i\n",
    "            pages = document.page_count\n",
    "    yield dataset[start_doc_i:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3645bee-c173-49aa-b301-2b4e1fd49b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainval = Dataset(\"trainval\", DATASET_PATH, load_annotations=False, load_ocr=False, cache_images=CachingConfig.OFF)\n",
    "\n",
    "max_pages_per_chunk = 2000\n",
    "\n",
    "for chunk in chunk_dataset(trainval, max_pages_per_chunk):\n",
    "    print(f\"{chunk}, pages: {chunk.total_page_count()}\")\n",
    "    chunk.load(annotations=True, ocr=False)\n",
    "    # ... work with the chunk here ...\n",
    "    chunk.release() # don't forget to free up the memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
