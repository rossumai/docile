{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ec8f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "from langchain.llms.base import LLM\n",
    "from typing import Optional, List, Mapping, Any\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "import os\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98522dca",
   "metadata": {},
   "source": [
    "# (1) Introduction: LangChain basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da51ee",
   "metadata": {},
   "source": [
    "## Definition of the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c282f3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:30<00:00, 15.05s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a6becc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    \n",
    "    tokenizer: T5Tokenizer\n",
    "    model: T5ForConditionalGeneration\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        outputs = self.model.generate(input_ids, \n",
    "                         do_sample=True, \n",
    "                         max_new_tokens=100, \n",
    "                         temperature=0.8)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d4f1a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = CustomLLM(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a35e0c5",
   "metadata": {},
   "source": [
    "## Create a prompt template\n",
    "You can create simple hardcoded prompts using the PromptTemplate class. Prompt templates can take any number of input variables, and can be formatted to generate a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "671989e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Socksify\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "I want you to act as a naming consultant for new companies.\n",
    "\n",
    "Here are some examples of good company names:\n",
    "\n",
    "- search engine, Google\n",
    "- social media, Facebook\n",
    "- video sharing, YouTube\n",
    "\n",
    "The name should be short, catchy and easy to remember.\n",
    "\n",
    "What is a good name for a company that makes {product}?\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=template,\n",
    ")\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "product = \"socks\"\n",
    "print(llm_chain.run(product=product))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c443604d",
   "metadata": {},
   "source": [
    "## Load a prompt template from LangChainHub\n",
    "\n",
    "LangChainHub contains a collection of prompts which can be loaded directly via LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90e0d3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `_type` key found, defaulting to `prompt`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of 1 + 1 is 2.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import load_prompt\n",
    "\n",
    "prompt_2 = load_prompt(\"lc://prompts/conversation/prompt.json\")\n",
    "\n",
    "llm_chain_2 = LLMChain(prompt=prompt_2, llm=llm)\n",
    "\n",
    "question = \"What is the result of 1 + 1?\"\n",
    "print(llm_chain_2.run(history=\"\", input=question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af7f8a7",
   "metadata": {},
   "source": [
    "## Pass few shot examples to a prompt template\n",
    "Few shot examples are a set of examples that can be used to help the language model generate a better response.\n",
    "\n",
    "To generate a prompt with few shot examples, you can use the FewShotPromptTemplate. This class takes in a PromptTemplate and a list of few shot examples. It then formats the prompt template with the few shot examples.\n",
    "\n",
    "In this example, we’ll create a prompt to generate word antonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65110416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benign\n"
     ]
    }
   ],
   "source": [
    "from langchain import PromptTemplate, FewShotPromptTemplate\n",
    "\n",
    "\n",
    "# First, create the list of few shot examples.\n",
    "examples = [\n",
    "    {\"word\": \"happy\", \"antonym\": \"sad\"},\n",
    "    {\"word\": \"tall\", \"antonym\": \"short\"},\n",
    "]\n",
    "\n",
    "# Next, we specify the template to format the examples we have provided.\n",
    "# We use the `PromptTemplate` class for this.\n",
    "example_formatter_template = \"\"\"\n",
    "Word: {word}\n",
    "Antonym: {antonym}\\n\n",
    "\"\"\"\n",
    "\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"word\", \"antonym\"],\n",
    "    template=example_formatter_template,\n",
    ")\n",
    "\n",
    "# Finally, we create the `FewShotPromptTemplate` object.\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    # These are the examples we want to insert into the prompt.\n",
    "    examples=examples,\n",
    "    # This is how we want to format the examples when we insert them into the prompt.\n",
    "    example_prompt=example_prompt,\n",
    "    # The prefix is some text that goes before the examples in the prompt.\n",
    "    # Usually, this consists of intructions.\n",
    "    prefix=\"Give the antonym of every input\",\n",
    "    # The suffix is some text that goes after the examples in the prompt.\n",
    "    # Usually, this is where the user input will go\n",
    "    suffix=\"Word: {input}\\nAntonym:\",\n",
    "    # The input variables are the variables that the overall prompt expects.\n",
    "    input_variables=[\"input\"],\n",
    "    # The example_separator is the string we will use to join the prefix, examples, and suffix together with.\n",
    "    example_separator=\"\\n\\n\",\n",
    ")\n",
    "\n",
    "llm_chain_3 = LLMChain(prompt=few_shot_prompt, llm=llm)\n",
    "\n",
    "word = \"monstruous\"\n",
    "print(llm_chain_3.run(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d448ce",
   "metadata": {},
   "source": [
    "# (2) Loading documents from our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fc17dd",
   "metadata": {},
   "source": [
    "## - Loading a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca96eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffc40b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_type = \"announcements\"\n",
    "email_number = \"002\"\n",
    "ex_email_path = \"./Dataset-v1/data/emails/email-\"+email_type+\"-\"+email_number+\".rtf\"\n",
    "\n",
    "loader = TextLoader(ex_email_path)\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af6b12f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"{\\\\rtf1\\\\ansi\\\\ansicpg1252\\\\cocoartf2638\\n\\\\cocoatextscaling0\\\\cocoaplatform0{\\\\fonttbl\\\\f0\\\\fnil\\\\fcharset0 HelveticaNeue;}\\n{\\\\colortbl;\\\\red255\\\\green255\\\\blue255;\\\\red42\\\\green49\\\\blue64;\\\\red245\\\\green245\\\\blue246;}\\n{\\\\*\\\\expandedcolortbl;;\\\\cssrgb\\\\c21569\\\\c25490\\\\c31765;\\\\cssrgb\\\\c96863\\\\c96863\\\\c97255;}\\n\\\\paperw11900\\\\paperh16840\\\\margl1440\\\\margr1440\\\\vieww11520\\\\viewh8400\\\\viewkind0\\n\\\\deftab720\\n\\\\pard\\\\pardeftab720\\\\sa400\\\\partightenfactor0\\n\\n\\\\f0\\\\fs32 \\\\cf2 \\\\cb3 \\\\expnd0\\\\expndtw0\\\\kerning0\\n\\\\outl0\\\\strokewidth0 \\\\strokec2 Subject: Update on Office Relocation\\\\\\nDear all,\\\\\\nAs you know, we are in the process of relocating our corporate office to a new and improved location. We are pleased to inform you that the move is going smoothly, and we are on track to complete the relocation by the end of this month.\\\\\\nOur new office is located at 456 Main Street, which is just a few blocks away from our current location. The new office space is larger and more modern, and it includes a range of amenities such as a fitness center, a caf\\\\'e9, and a rooftop terrace.\\\\\\nWe are excited about the move and the opportunities it presents for our company and our employees. We believe that the new office will provide a more comfortable and productive work environment, and it will help us to attract and retain top talent.\\\\\\nWe will be hosting a grand opening event on Friday, April 1st, from 4:00 pm to 7:00 pm, where you can tour the new office and enjoy some refreshments. Please RSVP to this email if you plan to attend the event.\\\\\\nThank you for your attention, and we look forward to welcoming you to our new office.\\\\\\nBest regards,\\\\\\nSarah Kim\\\\\\nOffice Manager\\\\\\n\\\\pard\\\\pardeftab720\\\\partightenfactor0\\n\\\\cf2 Pinnacle Enterprises\\\\\\n}\", lookup_str='', metadata={'source': './Dataset-v1/data/emails/email-announcements-002.rtf'}, lookup_index=0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9308ec3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3ef058678342778f95b276c4ad32dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "338fe524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLLM(LLM):\n",
    "    \n",
    "    tokenizer: T5Tokenizer\n",
    "    model: T5ForConditionalGeneration\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "    \n",
    "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
    "        if stop is not None:\n",
    "            raise ValueError(\"stop kwargs are not permitted.\")\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        outputs = self.model.generate(input_ids, \n",
    "                         do_sample=True, \n",
    "                         max_new_tokens=500, \n",
    "                         temperature=1e-10)\n",
    "        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "llm = CustomLLM(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c00cd",
   "metadata": {},
   "source": [
    "## - Summarization chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762a2590",
   "metadata": {},
   "source": [
    "### CombineDocuments Chains\n",
    "\n",
    "CombineDocuments chains are useful for when you need to run a language over multiple documents. Common use cases for this include question answering, question answering with sources, summarization, and more.\n",
    "\n",
    "**1) \"Stuffing\"**\n",
    "Stuffing is the simplest method, whereby you simply stuff all the related data into the prompt as context to pass to the language model. This is implemented in LangChain as the StuffDocumentsChain.\n",
    "\n",
    "Pros: Only makes a single call to the LLM. When generating text, the LLM has access to all the data at once.\n",
    "\n",
    "Cons: Most LLMs have a context length, and for large documents (or many documents) this will not work as it will result in a prompt larger than the context length.\n",
    "\n",
    "The main downside of this method is that it only works on smaller pieces of data. Once you are working with many pieces of data, this approach is no longer feasible. The next two approaches are designed to help deal with that.\n",
    "\n",
    "**2) \"Map Reduce\"**\n",
    "This method involves running an initial prompt on each chunk of data (for summarization tasks, this could be a summary of that chunk; for question-answering tasks, it could be an answer based solely on that chunk). Then a different prompt is run to combine all the initial outputs. This is implemented in the LangChain as the MapReduceDocumentsChain.\n",
    "\n",
    "Pros: Can scale to larger documents (and more documents) than StuffDocumentsChain. The calls to the LLM on individual documents are independent and can therefore be parallelized.\n",
    "\n",
    "Cons: Requires many more calls to the LLM than StuffDocumentsChain. Loses some information during the final combined call.\n",
    "\n",
    "\n",
    "**3) \"Refine\"**\n",
    "This method involves running an initial prompt on the first chunk of data, generating some output. For the remaining documents, that output is passed in, along with the next document, asking the LLM to refine the output based on the new document.\n",
    "\n",
    "Pros: Can pull in more relevant context, and may be less lossy than MapReduceDocumentsChain.\n",
    "\n",
    "Cons: Requires many more calls to the LLM than StuffDocumentsChain. The calls are also NOT independent, meaning they cannot be paralleled like MapReduceDocumentsChain. There is also some potential dependencies on the ordering of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c1de625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (591 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"We are in the process of relocating our corporate office to a new and improved location. We are pleased to inform you that the move is going smoothly, and we are on track to complete the relocation by the end of this month. Our new office is located at 456 Main Street, which is just a few blocks away from our current location. The new office space is larger and more modern, and it includes a range of amenities such as a fitness center, a caf'e9, and a rooftop terrace. We are excited about the move and the opportunities it presents for our company and our employees. We believe that the new office will provide a more comfortable and productive work environment, and it will help us to attract and retain top talent. We will be hosting a grand opening event on Friday, April 1st, from 4:00 pm to 7:00 pm, where you can tour the new office and enjoy some refreshments. Please RSVP to this email if you plan to attend the event. Thank you for your attention, and we look forward to welcoming you to our new office.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_mr = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
    "chain_mr.run(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd99b3d",
   "metadata": {},
   "source": [
    "## - Question-Answering chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16714c97",
   "metadata": {},
   "source": [
    "For question answering over many documents, you almost always want to create an index over the data. This can be used to smartly access the most relevant documents for a given question, allowing you to avoid having to pass all the documents to the LLM (saving you time and money)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e7bfcc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Chroma using direct local API.\n",
      "Using DuckDB in-memory for database. Data will be transient.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Friday, June 30th'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import VectorDBQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# Create your index\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "\n",
    "email_type = \"announcements\"\n",
    "texts = []\n",
    "for i in range(4): # Loading the first 4 emails\n",
    "    email_number = \"00\"+str(i)\n",
    "    ex_email_path = \"./Dataset-v1/data/emails/email-\"+email_type+\"-\"+email_number+\".rtf\"\n",
    "    loader = TextLoader(ex_email_path)\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "    texts.append(text_splitter.split_documents(documents)[0])\n",
    "    \n",
    "db = Chroma.from_documents(texts, embeddings)    \n",
    "qa = VectorDBQA.from_chain_type(llm=llm, chain_type=\"stuff\", vectorstore=db)\n",
    "\n",
    "# Create your query\n",
    "query = \"When is the deadline for registration for the Employee Training program?\" # Should output 30th June\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1200ed8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Maria Rodriguez'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Who is the sender of the email about the Employee Training program?\" # Maria Rodriguez\n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbb3e9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The move is going smoothly, and we are on track to complete the relocation by the end of this month.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What's the update on the office relocation topic?\" \n",
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e616330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'volunteering at local schools, participating in environmental clean-up events, and assisting with fundraising initiatives for local charities'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What will the volunteer program consist of?\"\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
